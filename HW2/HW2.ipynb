{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path =\"\"\n",
    "train_Data = pd.read_csv(path + \"source.csv\", header=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentence_list = train_Data.iloc[:,1].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_gram = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['景', '氣', '連', '3', '綠', '燈', '今', '年', 'GDP', '保', '三', '有', '望']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "def spliteKeyWord(str):\n",
    "    regex = r\"[\\u4e00-\\ufaff]|[0-9]+|[a-zA-Z]+\\'*[a-z]*\"\n",
    "    matches = re.findall(regex, str, re.UNICODE)\n",
    "    return matches\n",
    "print(spliteKeyWord('景氣連3綠燈 今年GDP保三有望'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building  n-gram Inverted Index "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now processing 1 gram\n",
      "Now processing 2 gram\n",
      "Now processing 3 gram\n",
      "--- Cost 10.432054042816162 seconds to build n-gram inverted index ---\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "n_gram_store = [] #  A list used to store every n_gram dictionary \n",
    "for i in range(n_gram):\n",
    "    gram_dict = {} # A dictionary used to contain set with n-gram index \n",
    "    i = i+1 \n",
    "    print(\"Now processing %s gram\" %(i))\n",
    "    #process every sentence in query \n",
    "    for sentence_index, sentence in enumerate(sentence_list):\n",
    "        \n",
    "        sentence = spliteKeyWord(sentence)\n",
    "        \n",
    "        #Start extract n-gram to gram_list:\n",
    "        for j in range(len(sentence) - (i-1)): # Preventing \"index out of range\" error\n",
    "        \n",
    "            # Concat char to get N-gram index\n",
    "            gram_index = ''.join(sentence[j:j+i])\n",
    "            \n",
    "            if gram_index not in gram_dict:\n",
    "                gram_dict[gram_index] = set([sentence_index+1]) # index_offset ->plus one \n",
    "            else : \n",
    "                gram_dict[gram_index].add(sentence_index+1) # index_offset ->plus one\n",
    "    n_gram_store.append(gram_dict)       \n",
    "    \n",
    "print(\"--- Cost %s seconds to build n-gram inverted index ---\" % (time.time() - start_time))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build inverted index by query "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "query_list = set()\n",
    "with open(path + \"query.txt\") as f:\n",
    "    queries = f.readlines()\n",
    "    for query in queries:\n",
    "        line = query.strip(\"\\n\").split(\" \")\n",
    "        for i, string in enumerate(line):\n",
    "            if i %2 == 0:\n",
    "                query_list.add(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Cost 0.30682921409606934 seconds to build query inverted index ---\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "query_store = [] \n",
    "query_dict = {} \n",
    "\n",
    "for query in (query_list):\n",
    "    #process every sentence in query \n",
    "    for sentence_index, sentence in enumerate(sentence_list):\n",
    "        if query in sentence:\n",
    "            if query not in query_dict:\n",
    "                query_dict[query] = set([sentence_index+1]) # index_offset ->plus one \n",
    "            else : \n",
    "                query_dict[query].add(sentence_index+1) # index_offset ->plus one       \n",
    "    \n",
    "print(\"--- Cost %s seconds to build query inverted index ---\" % (time.time() - start_time))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def English(str):\n",
    "    regex = r\"[a-zA-Z]+\\'*[a-z]*\"\n",
    "    matches = re.findall(regex, str, re.UNICODE)\n",
    "    if matches:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using n-gram inverted index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path + \"query.txt\") as f:\n",
    "    queries = f.readlines()\n",
    "    for query in queries:\n",
    "        line = query.strip(\"\\n\").split(\" \")\n",
    "        \n",
    "        if len(line) == 3:\n",
    "            query1 = line[0]\n",
    "            relation = line[1]\n",
    "            query2 = line[2]\n",
    "            if English(query1):\n",
    "                first_set = n_gram_store[0][query1]\n",
    "            else:\n",
    "                first_set = n_gram_store[len(query1)-1][query1]\n",
    "\n",
    "            if English(query2):\n",
    "                second_set = n_gram_store[0][query2]\n",
    "            else:\n",
    "                second_set = n_gram_store[len(query2)-1][query2]\n",
    "\n",
    "            if relation == 'and':\n",
    "                print(sorted(list(first_set & second_set)))\n",
    "            elif relation == 'or':\n",
    "                print(sorted(list(first_set | second_set)))\n",
    "            elif relation == 'not':\n",
    "                print(sorted(list(first_set - second_set)))\n",
    "        elif len(line)==5:\n",
    "            pass\n",
    "                    \n",
    "         \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using query inverted index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open(path + \"query.txt\") as f:\n",
    "    queries = f.readlines()\n",
    "    for query in queries:\n",
    "        line = query.strip(\"\\n\").split(\" \")\n",
    "        \n",
    "        if len(line) == 3:\n",
    "            query1 = line[0]\n",
    "            relation = line[1]\n",
    "            query2 = line[2]\n",
    "            first_set = query_dict[query1]\n",
    "            second_set = query_dict[query2]\n",
    "            if relation == 'and':\n",
    "                print(sorted(list(first_set & second_set)))\n",
    "            elif relation == 'or':\n",
    "                print(sorted(list(first_set | second_set)))\n",
    "            elif relation == 'not':\n",
    "                print(sorted(list(first_set - second_set)))\n",
    "        elif len(line)==5:\n",
    "            pass\n",
    "                    \n",
    "         \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "query1 = \"川普\"\n",
    "query2 = \"美國\"\n",
    "relation = \"and\"\n",
    "\n",
    "first_set = n_gram_store[len(query1)-1][query1]\n",
    "second_set = n_gram_store[len(query2)-1][query2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13, 678, 1225, 1905, 2756, 5218, 5839, 8690, 9409, 9640, 13064, 13961, 14156, 14186, 14481, 16770, 17606, 18090, 19914, 20105, 22030, 26636, 28769, 29177, 34604, 34995, 35007, 35463, 36249, 39440, 40672, 41297, 42578, 43556, 47121, 48139, 49384, 51361, 51696, 52696, 53362, 55588, 59223, 60384, 60388, 64906, 65735, 65739, 66178, 66507, 72616, 72838, 74469, 76031, 79419, 79806, 80345, 81268, 83306, 86922, 86944, 88801, 91921, 92423, 95643, 98797]\n"
     ]
    }
   ],
   "source": [
    "if relation ==\"and\": \n",
    "    print(sorted(list(first_set & second_set)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
